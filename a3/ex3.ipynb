{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "7044b324f88c4208d441c8fce3c8f6c454026eeaf7e810a2852062e3590255dc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "train_images = tf.image.resize(train_images, [32,32])\n",
    "test_images = tf.image.resize(test_images, [32,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 32, 32, 64)        640       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 32, 32, 64)        256       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 16, 16, 128)       73856     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 16, 16, 128)       512       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 8, 8, 256)         295168    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 8, 8, 256)         1024      \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 8, 8, 256)         590080    \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 8, 8, 256)         1024      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 4, 4, 512)         1180160   \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 4, 4, 512)         2048      \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 4, 4, 512)         2359808   \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 4, 4, 512)         2048      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 2, 2, 512)         2359808   \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 2, 2, 512)         2048      \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 2, 2, 512)         2359808   \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 2, 2, 512)         2048      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 1, 1, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 512)               0         \n_________________________________________________________________\ndense (Dense)                (None, 4096)              2101248   \n_________________________________________________________________\ndropout (Dropout)            (None, 4096)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 4096)              16781312  \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 4096)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                40970     \n=================================================================\nTotal params: 28,153,866\nTrainable params: 28,148,362\nNon-trainable params: 5,504\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 1), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2, strides=2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2, strides=2))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2, strides=2))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(2, strides=2))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(2, strides=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4929: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n",
      "1875/1875 [==============================] - 32s 15ms/step - loss: 0.2783 - accuracy: 0.9381 - val_loss: 0.2740 - val_accuracy: 0.9435\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1412 - accuracy: 0.9732 - val_loss: 0.0992 - val_accuracy: 0.9819\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1223 - accuracy: 0.9766 - val_loss: 0.0401 - val_accuracy: 0.9905\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1164 - accuracy: 0.9783 - val_loss: 0.0835 - val_accuracy: 0.9875\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0865 - accuracy: 0.9835 - val_loss: 0.0520 - val_accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_images, train_labels, epochs=5, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(\"vgg11 test accuracy.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.title('train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(\"vgg11 training accuracy.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'], label = 'test loss')\n",
    "plt.title('test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(\"vgg11 test loss.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.title('train loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig(\"vgg11 training loss.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'test accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"vgg11 Accuracy.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label = 'test loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"vgg11 Loss.png\")\n",
    "plt.close()"
   ]
  }
 ]
}